{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# volume\n",
    "from quid3 import QConnection, fetch\n",
    "from datetime import timedelta\n",
    "from datetime import datetime as dt\n",
    "from datetime import time\n",
    "from datetime import date\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from xbbg import blp\n",
    "from eqspydata.vendor import grdb\n",
    "import time\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "\n",
    "\n",
    "historic_interval = 6       #MINS    time bin for historical data - for 20 days avg\n",
    "intraday_interval = 3       #MINS    time bin for intraday data\n",
    "N_days_back = 33            #No. of days back we want to retrieve data from. This includes holidays too.\n",
    "No_day_avg = 20\n",
    "\n",
    "\n",
    "otas_grdb_exception = {\n",
    "    \" GY\": \" GR\",\n",
    "    \" SQ\": \" SM\",\n",
    "    \" SE\": \" SW\"\n",
    "}\n",
    "\n",
    "def get_otas_grdb_tickers(otas_tickers):\n",
    "    return [t.replace(key, value) for key, value in otas_grdb_exception.items() for t in otas_tickers]\n",
    "def get_security_data(tickers=None, rics=None, **kwargs):\n",
    "    if tickers:\n",
    "        symbols = get_otas_grdb_tickers(tickers)\n",
    "        security_data = grdb.get_security_master(symbols=symbols, **kwargs)\n",
    "    elif rics:\n",
    "        security_data = grdb.get_security_master(symbols=rics, symbol_type=\"ric\", **kwargs)\n",
    "    else:\n",
    "        raise ValueError()\n",
    "    return security_data['securities']\n",
    "def get_security_data_df(tickers, **kwargs):\n",
    "    securities = get_security_data(tickers = tickers, **kwargs)\n",
    "    # sym = \"`\" + '`'.join(self.total_member_trades['symbol'].unique().tolist())\n",
    "    # securities = pd.read_csv('http://grdbqa:8095/secmaster_query/current.csv?query={bloombergId:%20{$in:%20[%22VOD%20LN%22]},isPrimary:true}&include=bloombergId,ric,compositeRic,lnId,compositeLnId,exchangeMic,lotSize,currencyCode,mifid2Lis,tradingHoursCode,exchCountryCode,tickSizeSchema')\n",
    "    return pd.DataFrame(securities)\n",
    "\n",
    "def ticker_fx(tickers):\n",
    "    print(\"ticker_fx spread called\")\n",
    "    curr = pd.read_html(\"https://pool.liquidnet.com/api/result/state/6512/export?format=html\")[0][[\"LNCURRENCYCODE\", \"RATE_TO_USD\"]]\n",
    "    sec_data = get_security_data_df(tickers, fields=[\"primaryRic\", \"currencyCode\",'marketCapUsd']).drop_duplicates()\n",
    "    sec_data = sec_data.merge(curr, left_on=\"currencyCode\", right_on=\"LNCURRENCYCODE\", how=\"left\")\n",
    "    return sec_data\n",
    "\n",
    "# print(ticker_fx(['VOD LN']))\n",
    "\n",
    "#HERE INTERVAL IS MEASURED IN MINS\n",
    "def bars(interval,symlist,start_date,end_date,startTime,endTime):               # N_Days is number of market days before most recent trading day\n",
    "    with QConnection(app='test', user='tsundar') as conn:\n",
    "        current = dt.now()\n",
    "        df = fetch(\n",
    "            conn,\n",
    "            'bar',\n",
    "            dict(\n",
    "                Type = 'ric',\n",
    "                symlist=symlist,\n",
    "                startDate = start_date.date(),\n",
    "                endDate = end_date.date(),\n",
    "                barsz = interval,               #interval must be given in an integer number of minutes. Interval is measured in seconds so it must given in multiples of 60\n",
    "                volumeCol = 'TotalVolume',\n",
    "                auctionExtension = True,\n",
    "                startTime = startTime,\n",
    "                endTime = endTime             #find volume to the current time\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "    df['time'] = df['time'] + timedelta(hours=1)\n",
    "    df['date_time'] = df['date']+df['time']\n",
    "    df = df[['AverageSpreadbps', 'date_time','sym','Value']]\n",
    "    # df = df.merge(ticker_fx(symList), left_on= ['sym'],right_on=['primaryRic'],how= 'left')\n",
    "\n",
    "    df['AverageSpreadbps'][df['AverageSpreadbps']<0] = 0\n",
    "    df = df[df['date_time']<=current]\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def bars_prev(interval, symList,start_date,end_date,startTime,endTime):               # N_Days is number of market days before most recent trading day\n",
    "    with QConnection(app='test', user='tsundar') as conn:\n",
    "        current = dt.now()\n",
    "        df = fetch(\n",
    "            conn,\n",
    "            'bar',\n",
    "            dict(\n",
    "                Type = 'ric',\n",
    "                symlist=symList,\n",
    "                startDate = start_date.date(),\n",
    "                endDate = end_date.date(),\n",
    "                barsz = interval,               #interval must be given in an integer number of minutes. Interval is measured in seconds so it must given in multiples of 60\n",
    "                volumeCol = 'TotalVolume',\n",
    "                auctionExtension = True,\n",
    "                startTime = startTime,\n",
    "                endTime = endTime             #find volume to the current time\n",
    "            )\n",
    "        )\n",
    "\n",
    "    df['time'] = df['time'] + timedelta(hours=1)\n",
    "    df['date_time'] = df['date']+df['time']\n",
    "    df = df[['AverageSpreadbps','sym','date_time','time','date','Value']]\n",
    "    df['AverageSpreadbps'][df['AverageSpreadbps']<0] = 0\n",
    "    df['AverageSpreadbps'] = df['AverageSpreadbps'].fillna(0)\n",
    "\n",
    "    return df\n",
    "\n",
    "def value_weighted_spread(df):\n",
    "    # single value for avg spread\n",
    "    total_val = df[\"Value$\"].sum()\n",
    "    if total_val != 0:\n",
    "        w_spread =(df['Value$'].multiply(df['AverageSpreadbps'])).sum()/total_val\n",
    "    else:\n",
    "        w_spread = 0\n",
    "\n",
    "    # try:\n",
    "    #     w_spread =(df['Value$'].multiply(df['AverageSpreadbps'])).sum()/total_val\n",
    "    # except RuntimeWarning:\n",
    "    #     w_spread = 0\n",
    "\n",
    "    return pd.Series({\"value_weightedSpread_bps\": w_spread})\n",
    "\n",
    "def mcap_weighted_spread(df):\n",
    "    total_mcap = df['marketCapUsd'].sum()\n",
    "    if total_mcap != 0:\n",
    "        w_spread =(df['marketCapUsd'].multiply(df['AverageSpreadbps'])).sum()/total_mcap\n",
    "    else:\n",
    "        w_spread = 0\n",
    "\n",
    "    return pd.Series({\"mcap_weightedSpread_bps\": w_spread})\n",
    "\n",
    "\n",
    "\n",
    "class Spread:\n",
    "\n",
    "    def __init__(self,names):\n",
    "        self.names = names\n",
    "        self.rics = ticker_fx(self.names)\n",
    "        self.intraday = self.intraday_loader()          #gives the intraday volume - note this does not cumsum. need to call the intraday updater to do this                                                                               #this just gives the 20 day average for particular names passed in      #I'M NOT SURE WE ACTUALLY NEED THIS FUNCTION\n",
    "        self.historic = self.historic_loader()\n",
    "        print(\"initialized spread\")\n",
    "\n",
    "    def update_name_list(self, new_names):\n",
    "        try:\n",
    "            self.names = new_names\n",
    "            self.rics = ticker_fx(new_names)\n",
    "            self.intraday = self.intraday_loader()\n",
    "            self.historic = self.historic_loader()\n",
    "            return True\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "    def intraday_loader(self):\n",
    "\n",
    "        intra = bars(interval=intraday_interval, symlist=self.rics['primaryRic'].tolist(), start_date=dt.now(),\n",
    "                     end_date=dt.now(), startTime=dt(2023, 7, 17, 7).time(), endTime=dt.now().time())\n",
    "        # Now I want a df which combines both the intra dataframe as well as parts of the rics dataframe\n",
    "        intra = intra.merge(self.rics, left_on='sym', right_on='primaryRic', how='left')\n",
    "        # intra = intra['Value','date_time','sym','RATE_TO_USD']\n",
    "        intra['Value$'] = intra['RATE_TO_USD'] * intra['Value'].fillna(method='ffill')\n",
    "        final_df = intra.groupby('date_time').apply(mcap_weighted_spread)  # take the groupby out of here and put it into the intraday updater function\n",
    "        final_df['value_weightedSpread_bps'] = intra.groupby('date_time').apply(value_weighted_spread)\n",
    "\n",
    "        return final_df.reset_index()\n",
    "\n",
    "    def intraday_updater(self):                 # this function again takes in the names we care about, then make a dataframe of the volume data from the last time of the intra df to the current time. it then concatenates it onto the end of the intraday variable. this way, we dont have to load all of the previous days data.\n",
    "        if (dt.now().time() > dt(2023, 8, 14, 8).time()):\n",
    "\n",
    "\n",
    "            last_datetime = max(self.intraday['date_time'])\n",
    "            next = bars(interval=intraday_interval, symlist=self.rics['primaryRic'].tolist(), start_date=last_datetime,end_date=last_datetime, startTime=((last_datetime) - timedelta(hours=1)).time(),endTime=(dt.now() - timedelta(hours=1)).time())\n",
    "            if len(next) == 0:\n",
    "                return self.intraday\n",
    "                       #accounting for the UTC conversion as in the bars function we added on 1 hour anyway. so here we take off an hour.\n",
    "\n",
    "\n",
    "            ## take max value of datetime column, then add 2 mins, then concat to self.intraday\n",
    "            else:\n",
    "                next = next.merge(self.rics, left_on='sym', right_on='primaryRic', how='left')\n",
    "                next['Value$'] = next['RATE_TO_USD'] * next['Value'].fillna(method = 'ffill')\n",
    "                next_final = next.groupby('date_time').apply(mcap_weighted_spread).reset_index()\n",
    "                next_final['Value_weighted'] = next.groupby('date_time').apply(value_weighted_spread)\n",
    "\n",
    "                self.intraday = pd.concat([self.intraday,next_final])\n",
    "                # self.intraday['cum_vol$'] = self.intraday['Value$'].fillna(0).cumsum()\n",
    "                self.intraday = self.intraday[~self.intraday['date_time'].duplicated(keep = 'first')]           #getting rid of duplicated times contaminating the cumsum\n",
    "                # self.intraday = self.intraday[self.intraday['date_time'].dt.time<=dt(2023,7,23,17).time()]\n",
    "                # self.intraday = max(self.intraday['date_time'].time)\n",
    "                self.intraday['mcap_weightedSpread_bps'] = self.intraday['mcap_weightedSpread_bps'].replace(0,pd.NA).fillna(method = 'ffill')\n",
    "                self.intraday['value_weightedSpread_bps'] = self.intraday['value_weightedSpread_bps'].replace(0,pd.NA).fillna(method = 'ffill')\n",
    "                self.intraday= self.intraday[(self.intraday['date_time'].dt.time)<=dt(2023,7,23,16,30).time()]\n",
    "                return self.intraday\n",
    "\n",
    "        else:\n",
    "            last_trade_day = self.historic[self.historic['date'] == self.historic['date'].max()]\n",
    "            # last_trade_day = last_trade_day.merge(self.rics, left_on='sym', right_on='primaryRic', how='left')\n",
    "            last_trade_day['Value$'] = last_trade_day['RATE_TO_USD'] * last_trade_day['Value'].fillna(method='ffill')\n",
    "            final_df = pd.DataFrame()\n",
    "            final_df['mcap_weightedSpread_bps'] = last_trade_day.groupby('date_time').apply(mcap_weighted_spread)  # take the groupby out of here and put it into the intraday updater function\n",
    "            final_df['value_weightedSpread_bps'] = last_trade_day.groupby('date_time').apply(value_weighted_spread)\n",
    "            final_df = final_df.reset_index()\n",
    "            final_df['time'] = final_df['date_time'].dt.time\n",
    "            final_df[final_df['time'] <= dt(2023, 7, 23, 16, 30).time()]\n",
    "            # final_df = final_df[(final_df['date_time'] + dt(2023, 7, 23)).dt.time <= dt(2023, 7, 23, 16, 30).time()]\n",
    "            self.intraday = final_df\n",
    "\n",
    "\n",
    "            return self.intraday\n",
    "\n",
    "\n",
    "\n",
    "    def historic_loader(self):\n",
    "        # rics = ticker_fx(self.names)\n",
    "        prev = bars_prev(interval=historic_interval, symList=self.rics['primaryRic'].tolist(),\n",
    "                         start_date=(dt.now() - timedelta(31)), end_date=(dt.now() - timedelta(days=1)),\n",
    "                         startTime=dt(2023, 7, 17, 7).time(), endTime=dt(2023, 7, 17, 16).time())\n",
    "        first_day = prev['date'].unique()[-20]\n",
    "        prev = prev[prev['date'] >= first_day]\n",
    "        prev = prev.merge(self.rics, left_on='sym', right_on='primaryRic', how='left')\n",
    "        return prev\n",
    "\n",
    "\n",
    "    def historic_updater(self):\n",
    "        current = dt.now()\n",
    "        self.historic = self.historic[self.historic['date_time'] >= (current - timedelta(days=No_day_avg))]\n",
    "        # rics = ticker_fx(self.names)\n",
    "\n",
    "        if max(self.historic['date_time']).date() != (dt.now() - timedelta(days=1)).date():  # checking whether yesterday's data has been added to the self.historics dataframe\n",
    "\n",
    "            days_back = 1\n",
    "            yesterday = bars_prev(interval=historic_interval, symList=self.rics['primaryRic'].tolist(),\n",
    "                                  start_date=dt.now() - timedelta(days=days_back),\n",
    "                                  end_date=dt.now() - timedelta(days=days_back),\n",
    "                                  startTime=dt(2023, 7, 24, 7).time(), endTime=dt(2023, 7, 24, 16).time())\n",
    "\n",
    "            while len(yesterday) != 0:\n",
    "                days_back += 1\n",
    "                yesterday = bars_prev(interval=historic_interval, symList=self.rics['primaryRic'].tolist(),\n",
    "                                      start_date=dt.now() - timedelta(days=days_back),\n",
    "                                      end_date=dt.now() - timedelta(days=days_back),\n",
    "                                      startTime=dt(2023, 7, 24, 7).time(), endTime=dt(2023, 7, 24, 16).time())\n",
    "\n",
    "            yesterday = yesterday.merge(self.rics, left_on='sym', right_on='primaryRic',how='left')  # joining rics on here. we need to do this here so that the yesterday dataframe is in the same form as the sself.historic dataframe. so that we dont mess it up when we concatenate\n",
    "            self.historic = pd.concat([self.historic, yesterday])  # here we concat onto the end of self.historic\n",
    "\n",
    "        self.historic['Value$'] = self.historic['RATE_TO_USD'] * self.historic['Value']  # get dollar value\n",
    "\n",
    "        return self.historic.sort_values(\"date_time\")\n",
    "\n",
    "\n",
    "    def historic_average(self):\n",
    "        t_historic = self.historic_updater()  # historic updater gets called when we click the historic average function\n",
    "        t_historic = t_historic.groupby(['sym', 'time'], as_index=False, sort=False, dropna=False).mean()       #this line returns the mean values for each unique combination of 'sym' and 'time'\n",
    "        t_historic_mcap = t_historic.groupby('time', as_index=True, sort=False, dropna=False).apply(mcap_weighted_spread)  #I have made time the index here\n",
    "        t_historic_value = t_historic.groupby('time', as_index = True).apply(value_weighted_spread)\n",
    "        final = t_historic_mcap.merge(t_historic_value, left_index=True, right_index=True,how = 'inner')\n",
    "        # t_historic_final = t_historic_final.sort_values('time')  # not sure why we sort vals by time here tbh   # This is future TJ I think I've figured it out. Its because the time column isn't in chronological order.\n",
    "        # t_historic_final = t_historic_final.fillna(method='ffill')\n",
    "        # t_historic_final = t_historic_final[(t_historic_final['time']+dt(2023,7,23)).dt.time<=dt(2023,7,23,16,30).time()]\n",
    "        final = final.sort_index()\n",
    "        # final = final.sort_values('time')\n",
    "        final = final.fillna(method='ffill').reset_index()\n",
    "        final = final[(final['time']+dt(2023,7,23)).dt.time<=dt(2023,7,23,16,30).time()]\n",
    "\n",
    "        return final\n",
    "\n",
    "\n",
    "\n",
    "olume\n",
    "\n",
    "from quid3 import QConnection, fetch\n",
    "from datetime import timedelta,time,date\n",
    "from datetime import datetime as dt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from xbbg import blp\n",
    "from eqspydata.vendor import grdb\n",
    "import time\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "historic_interval = 6       #MINS    time bin for historical data - for 20 days avg\n",
    "intraday_interval = 3       #MINS    time bin for intraday data\n",
    "N_days_back = 32            #No. of days back we want to retrieve data from. This includes holidays too.\n",
    "No_day_avg = 20\n",
    "\n",
    "\n",
    "\n",
    "otas_grdb_exception = {\n",
    "    \" GY\": \" GR\",\n",
    "    \" SQ\": \" SM\",\n",
    "    \" SE\": \" SW\"\n",
    "}\n",
    "\n",
    "def get_otas_grdb_tickers(otas_tickers):\n",
    "    return [t.replace(key, value) for key, value in otas_grdb_exception.items() for t in otas_tickers]\n",
    "def get_security_data(tickers=None, rics=None, **kwargs):\n",
    "    if tickers:\n",
    "        symbols = get_otas_grdb_tickers(tickers)\n",
    "        security_data = grdb.get_security_master(symbols=symbols, **kwargs)\n",
    "    elif rics:\n",
    "        security_data = grdb.get_security_master(symbols=rics, symbol_type=\"ric\", **kwargs)\n",
    "    else:\n",
    "        raise ValueError()\n",
    "    return security_data['securities']\n",
    "def get_security_data_df(tickers, **kwargs):\n",
    "    securities = get_security_data(tickers = tickers, **kwargs)\n",
    "    return pd.DataFrame(securities)\n",
    "\n",
    "def ticker_fx(tickers):\n",
    "    print(\"ticker_fx main called\")\n",
    "    curr = pd.read_html(\"https://pool.liquidnet.com/api/result/state/6512/export?format=html\")[0][[\"LNCURRENCYCODE\", \"RATE_TO_USD\"]]\n",
    "    sec_data = get_security_data_df(tickers, fields=[\"compositeRic\", \"currencyCode\",'marketCap']).drop_duplicates()\n",
    "    sec_data = sec_data.merge(curr, left_on=\"currencyCode\", right_on=\"LNCURRENCYCODE\", how=\"left\")\n",
    "    return sec_data\n",
    "\n",
    "# print(ticker_fx(['VOD LN']))\n",
    "\n",
    "#HERE INTERVAL IS MEASURED IN MINS\n",
    "def bars(interval,symlist,start_date,end_date,startTime,endTime):               # N_Days is number of market days before most recent trading day\n",
    "    with QConnection(app='test', user='tsundar') as conn:\n",
    "        current = dt.now()\n",
    "        df = fetch(\n",
    "            conn,\n",
    "            'bar',\n",
    "            dict(\n",
    "                Type = 'ric',\n",
    "                symlist=symlist,\n",
    "                startDate = start_date.date(),\n",
    "                endDate = end_date.date(),\n",
    "                barsz = interval,               #interval must be given in an integer number of minutes. Interval is measured in seconds so it must given in multiples of 60\n",
    "                volumeCol = 'TotalVolume',\n",
    "                auctionExtension = True,\n",
    "                startTime = startTime,\n",
    "                endTime = endTime             #find volume to the current time\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "    df['time'] = df['time'] + timedelta(hours=1)                #for UTC-BST conversion\n",
    "    df['date_time'] = df['date']+df['time']\n",
    "    df = df[['Value', 'date_time','sym','Volume']]\n",
    "    # df = df.merge(ticker_fx(symList), left_on= ['sym'],right_on=['primaryRic'],how= 'left')\n",
    "\n",
    "    df['Value'][df['Value']<0] = 0\n",
    "    df['Volume'][df['Volume']<0] = 0\n",
    "    df = df[df['date_time']<=current]\n",
    "    return df\n",
    "\n",
    "\n",
    "def bars_prev(interval, symList,start_date,end_date,startTime,endTime):               # N_Days is number of market days before most recent trading day\n",
    "    with QConnection(app='test', user='tsundar') as conn:\n",
    "        current = dt.now()\n",
    "        df = fetch(\n",
    "            conn,\n",
    "            'bar',\n",
    "            dict(\n",
    "                Type = 'ric',\n",
    "                symlist=symList,\n",
    "                startDate = start_date.date(),\n",
    "                endDate = end_date.date(),\n",
    "                barsz = interval,               #interval must be given in an integer number of minutes. Interval is measured in seconds so it must given in multiples of 60\n",
    "                volumeCol = 'TotalVolume',\n",
    "                auctionExtension = True,\n",
    "                startTime = startTime,\n",
    "                endTime = endTime             #find volume to the current time\n",
    "            )\n",
    "        )\n",
    "\n",
    "    df['time'] = df['time'] + timedelta(hours=1)\n",
    "    df['date_time'] = df['date']+df['time']\n",
    "    df = df[['Value','sym','date_time','time','date','Volume']]\n",
    "    df['Value'][df['Value']<0] = 0\n",
    "    df['Volume'][df['Volume']<0] = 0\n",
    "    # df = df[df['date_time']<=current]\n",
    "    # df = df.groupby('time', as_index=False).mean()\n",
    "    # df['cum_vol'] = df['Value'].fillna(0)\n",
    "    df['Volume'] = df['Volume'].fillna(0)\n",
    "    df['Value'] = df['Value'].fillna(0)\n",
    "\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "class Volume:\n",
    "\n",
    "    def __init__(self,names):\n",
    "        self.names = names\n",
    "        self.rics = ticker_fx(self.names)\n",
    "        self.intraday = self.intraday_loader()          #gives the intraday volume - note this does not cumsum. need to call the intraday updater to do this                                                                               #this just gives the 20 day average for particular names passed in      #I'M NOT SURE WE ACTUALLY NEED THIS FUNCTION\n",
    "        self.historic = self.historic_loader()          #this function loads the last 20 days data. no groupbys or anything, just the raw data.\n",
    "        print(\"initialized Volume\")\n",
    "        # self.rics = ticker\n",
    "        # self.updated_day = date.today()\n",
    "        # self.average = self.historic_average()\n",
    "\n",
    "    def update_name_list(self, new_names):\n",
    "        try:\n",
    "            self.names = new_names\n",
    "            self.rics = ticker_fx(new_names)\n",
    "            self.intraday = self.intraday_loader()\n",
    "            self.historic = self.historic_loader()\n",
    "            return True\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "\n",
    "    def intraday_loader(self):\n",
    "        intra = bars(interval=intraday_interval, symlist=self.rics['compositeRic'].tolist(), start_date=dt.now(),\n",
    "                     end_date=dt.now(), startTime=dt(2023, 7, 17, 7).time(), endTime=dt.now().time())\n",
    "        # Now I want a df which combines both the intra dataframe as well as parts of the rics dataframe\n",
    "        intra = intra.merge(self.rics, left_on='sym', right_on='compositeRic', how='left')\n",
    "        # intra = intra['Value','date_time','sym','RATE_TO_USD']\n",
    "        intra['Value$'] = intra['RATE_TO_USD'] * intra['Value']\n",
    "        intra = intra.groupby('date_time', as_index=False).sum()\n",
    "\n",
    "        # intraday = intra(self.names)          # this function gives the intraday volume from 8am to the current time. This then gets stored in the variable intraday.\n",
    "        return intra\n",
    "\n",
    "    def intraday_updater(self):                 # this function again takes in the names we care about, then make a dataframe of the volume data from the last time of the intra df to the current time. it then concatenates it onto the end of the intraday variable. this way, we dont have to load all of the previous days data.\n",
    "\n",
    "\n",
    "\n",
    "        if (dt.now().time() > dt(2023, 8, 14, 8).time()):\n",
    "            # rics = ticker_fx(self.names)\n",
    "            last_datetime = max(self.intraday['date_time'])\n",
    "            next = bars(interval=intraday_interval, symlist=self.rics['compositeRic'].tolist(),start_date=last_datetime, end_date=last_datetime,startTime=((last_datetime) - timedelta(hours=1)).time(), endTime=(dt.now() - timedelta(hours=1)).time())  # accounting for the UTC conversion as in the bars function we added on 1 hour anyway. so here we take off an hour.\n",
    "\n",
    "            ## take max value of datetime column, then add 2 mins, then concat to self.intraday\n",
    "            next = next.merge(self.rics, left_on='sym', right_on='compositeRic', how='left')\n",
    "            next['Value$'] = next['RATE_TO_USD'] * next['Value']\n",
    "            next = next.groupby('date_time', as_index=False).sum()\n",
    "            self.intraday = pd.concat([self.intraday, next])\n",
    "            self.intraday['cum_vol$'] = self.intraday['Value$'].fillna(0).cumsum()\n",
    "            self.intraday['cum_vol'] = self.intraday['Volume'].fillna(0).cumsum()\n",
    "            self.intraday = self.intraday[~self.intraday['date_time'].duplicated(\n",
    "                keep='first')]  # getting rid of duplicated times contaminating the cumsum\n",
    "            self.intraday = self.intraday[self.intraday['date_time'].dt.time <= dt(2023, 7, 23, 17).time()]\n",
    "            # self.intraday = max(self.intraday['date_time'].time)\n",
    "            return self.intraday\n",
    "        else:\n",
    "            last_trade_day = self.historic[self.historic['date'] == self.historic['date'].max()]\n",
    "            # last_trade_day.merge(self.rics, left_on='sym', right_on='compositeRic', how='left')\n",
    "            last_trade_day['Value$'] = last_trade_day['RATE_TO_USD'] * last_trade_day['Value']\n",
    "            last_trade_day = last_trade_day.groupby('date_time', as_index=False).sum()\n",
    "            last_trade_day['cum_vol$'] = last_trade_day['Value$'].fillna(0).cumsum()\n",
    "            last_trade_day['cum_vol'] = last_trade_day['Volume'].fillna(0).cumsum()\n",
    "            # last_trade_day = last_trade_day[~last_trade_day['date_time'].duplicated(keep='first')]\n",
    "            # last_trade_day = last_trade_day[last_trade_day['date_time'].dt.time <= dt(2023, 7, 23, 17).time()]\n",
    "            self.intraday = last_trade_day.reset_index()\n",
    "            # last_trade_day = last_trade_day.groupby('time', as_index=False).sum()\n",
    "            return self.intraday\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def historic_loader(self):\n",
    "        # rics = ticker_fx(self.names)\n",
    "        prev = bars_prev(interval=historic_interval, symList=self.rics['compositeRic'].tolist(), start_date=(dt.now()) - timedelta(31),end_date=(dt.now()-timedelta(days = 1)), startTime=dt(2023, 7, 17, 7).time(), endTime=dt(2023, 7, 17, 16).time())          #this gives all the data for the last 30 days. not including today.so it gives data from (t-21) to (t-1). also, this includes weekends and non trading days.\n",
    "        first_day = prev['date'].unique()[-20]          #this first day variable takes the prev df, and looks at the date column. .unique, sorts gets rid of duplicates and sorts it too. picking out the -20th element will take the lat 20th trading day we care about.\n",
    "        prev = prev[prev['date'] >= first_day]          # only keep rows of the df where the date is after the last 20 trading day. get rid of all the excess ones we have loaded.\n",
    "\n",
    "        prev = prev.merge(self.rics, left_on='sym', right_on='compositeRic', how='left')\n",
    "\n",
    "        # Ideally, join rics to the prev df, then u got it in hisoric-updater and all the other good stuff;\n",
    "\n",
    "        return prev\n",
    "\n",
    "    def historic_updater(self):         # this function must take in the names and then if yesterday's data hasn't been uploaded, then upload it. also if the first day in the list is more than 20 days in the past, then remove that day's data from the list\n",
    "        current = dt.now()\n",
    "\n",
    "        self.historic = self.historic[self.historic['date_time'] >= (current-timedelta(days = No_day_avg))]                #get rid of anything which isnt 20 days ago. this will get rid of the 21 day old data.\n",
    "        # rics = ticker_fx(self.names)\n",
    "        if max(self.historic['date_time']).date() != (dt.now()-timedelta(days = 1)).date():             #checking whether yesterday's data has been added to the self.historics dataframe\n",
    "\n",
    "            days_back = 1\n",
    "            yesterday = bars_prev(interval=historic_interval, symList=self.rics['compositeRic'].tolist(),\n",
    "                                  start_date=dt.now() - timedelta(days=days_back), end_date=dt.now() - timedelta(days=days_back),\n",
    "                                  startTime=dt(2023, 7, 24, 7).time(), endTime=dt(2023, 7, 24, 16).time())\n",
    "\n",
    "            while len(yesterday) != 0:\n",
    "                days_back+=1\n",
    "                yesterday = bars_prev(interval=historic_interval, symList=self.rics['compositeRic'].tolist(),\n",
    "                                      start_date=dt.now() - timedelta(days=days_back), end_date=dt.now() - timedelta(days=days_back),\n",
    "                                      startTime=dt(2023, 7, 24, 7).time(), endTime=dt(2023, 7, 24, 16).time())\n",
    "\n",
    "            yesterday = yesterday.merge(self.rics, left_on='sym', right_on='compositeRic', how='left')       # joining rics on here. we need to do this here so that the yesterday dataframe is in the same form as the sself.historic dataframe. so that we dont mess it up when we concatenate\n",
    "\n",
    "\n",
    "            self.historic = pd.concat([self.historic, yesterday])                                       #here we concat onto the end of self.historic\n",
    "\n",
    "        self.historic['Value$'] = self.historic['RATE_TO_USD'] * self.historic['Value']                 #get dollar value\n",
    "\n",
    "        return self.historic.sort_values(\"date_time\")\n",
    "\n",
    "\n",
    "    def historic_average(self):          # this takes the historic data, groups by the stock symbol, takes an average, groups by time, takes a sum\n",
    "        t_historic = self.historic_updater()                                                                                #this gets called when we click the historic average function\n",
    "        t_historic = t_historic.groupby(['sym', 'time'], as_index=False, sort=False, dropna=False).mean()\n",
    "        t_historic = t_historic.groupby('time', as_index=False, sort=False, dropna=False).sum()\n",
    "        t_historic = t_historic.sort_values('time')         #not sure why we sort vals by time here\n",
    "\n",
    "        t_historic['cum_vol$'] = t_historic['Value$'].fillna(0).cumsum()\n",
    "        t_historic['cum_vol'] = t_historic['Volume'].fillna(0).cumsum()\n",
    "        return t_historic\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
