{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spread:\n",
    "from quid3 import QConnection, fetch\n",
    "from datetime import timedelta\n",
    "from datetime import datetime as dt\n",
    "from datetime import time\n",
    "from datetime import date\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from xbbg import blp\n",
    "from eqspydata.vendor import grdb\n",
    "import time\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "\n",
    "\n",
    "historic_interval = 6       #MINS    time bin for historical data - for 20 days avg\n",
    "intraday_interval = 3       #MINS    time bin for intraday data\n",
    "N_days_back = 33            #No. of days back we want to retrieve data from. This includes holidays too.\n",
    "No_day_avg = 20\n",
    "\n",
    "\n",
    "otas_grdb_exception = {\n",
    "    \" GY\": \" GR\",\n",
    "    \" SQ\": \" SM\",\n",
    "    \" SE\": \" SW\"\n",
    "}\n",
    "\n",
    "def get_otas_grdb_tickers(otas_tickers):\n",
    "    return [t.replace(key, value) for key, value in otas_grdb_exception.items() for t in otas_tickers]\n",
    "def get_security_data(tickers=None, rics=None, **kwargs):\n",
    "    if tickers:\n",
    "        symbols = get_otas_grdb_tickers(tickers)\n",
    "        security_data = grdb.get_security_master(symbols=symbols, **kwargs)\n",
    "    elif rics:\n",
    "        security_data = grdb.get_security_master(symbols=rics, symbol_type=\"ric\", **kwargs)\n",
    "    else:\n",
    "        raise ValueError()\n",
    "    return security_data['securities']\n",
    "def get_security_data_df(tickers, **kwargs):\n",
    "    securities = get_security_data(tickers = tickers, **kwargs)\n",
    "    # sym = \"`\" + '`'.join(self.total_member_trades['symbol'].unique().tolist())\n",
    "    # securities = pd.read_csv('http://grdbqa:8095/secmaster_query/current.csv?query={bloombergId:%20{$in:%20[%22VOD%20LN%22]},isPrimary:true}&include=bloombergId,ric,compositeRic,lnId,compositeLnId,exchangeMic,lotSize,currencyCode,mifid2Lis,tradingHoursCode,exchCountryCode,tickSizeSchema')\n",
    "    return pd.DataFrame(securities)\n",
    "\n",
    "def ticker_fx(tickers):\n",
    "    print(\"ticker_fx spread called\")\n",
    "    curr = pd.read_html(\"https://pool.liquidnet.com/api/result/state/6512/export?format=html\")[0][[\"LNCURRENCYCODE\", \"RATE_TO_USD\"]]\n",
    "    sec_data = get_security_data_df(tickers, fields=[\"primaryRic\", \"currencyCode\",'marketCapUsd']).drop_duplicates()\n",
    "    sec_data = sec_data.merge(curr, left_on=\"currencyCode\", right_on=\"LNCURRENCYCODE\", how=\"left\")\n",
    "    return sec_data\n",
    "\n",
    "# print(ticker_fx(['VOD LN']))\n",
    "\n",
    "#HERE INTERVAL IS MEASURED IN MINS\n",
    "def bars(interval,symlist,start_date,end_date,startTime,endTime):               # N_Days is number of market days before most recent trading day\n",
    "    with QConnection(app='test', user='tsundar') as conn:\n",
    "        current = dt.now()\n",
    "        df = fetch(\n",
    "            conn,\n",
    "            'bar',\n",
    "            dict(\n",
    "                Type = 'ric',\n",
    "                symlist=symlist,\n",
    "                startDate = start_date.date(),\n",
    "                endDate = end_date.date(),\n",
    "                barsz = interval,               #interval must be given in an integer number of minutes. Interval is measured in seconds so it must given in multiples of 60\n",
    "                volumeCol = 'TotalVolume',\n",
    "                auctionExtension = True,\n",
    "                startTime = startTime,\n",
    "                endTime = endTime             #find volume to the current time\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "    df['time'] = df['time'] + timedelta(hours=1)\n",
    "    df['date_time'] = df['date']+df['time']\n",
    "    df = df[['AverageSpreadbps', 'date_time','sym','Value']]\n",
    "    # df = df.merge(ticker_fx(symList), left_on= ['sym'],right_on=['primaryRic'],how= 'left')\n",
    "\n",
    "    df['AverageSpreadbps'][df['AverageSpreadbps']<0] = 0\n",
    "    df = df[df['date_time']<=current]\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def bars_prev(interval, symList,start_date,end_date,startTime,endTime):               # N_Days is number of market days before most recent trading day\n",
    "    with QConnection(app='test', user='tsundar') as conn:\n",
    "        current = dt.now()\n",
    "        df = fetch(\n",
    "            conn,\n",
    "            'bar',\n",
    "            dict(\n",
    "                Type = 'ric',\n",
    "                symlist=symList,\n",
    "                startDate = start_date.date(),\n",
    "                endDate = end_date.date(),\n",
    "                barsz = interval,               #interval must be given in an integer number of minutes. Interval is measured in seconds so it must given in multiples of 60\n",
    "                volumeCol = 'TotalVolume',\n",
    "                auctionExtension = True,\n",
    "                startTime = startTime,\n",
    "                endTime = endTime             #find volume to the current time\n",
    "            )\n",
    "        )\n",
    "\n",
    "    df['time'] = df['time'] + timedelta(hours=1)\n",
    "    df['date_time'] = df['date']+df['time']\n",
    "    df = df[['AverageSpreadbps','sym','date_time','time','date','Value']]\n",
    "    df['AverageSpreadbps'][df['AverageSpreadbps']<0] = 0\n",
    "    df['AverageSpreadbps'] = df['AverageSpreadbps'].fillna(0)\n",
    "\n",
    "    return df\n",
    "\n",
    "def value_weighted_spread(df):\n",
    "    # single value for avg spread\n",
    "    total_val = df[\"Value$\"].sum()\n",
    "    if total_val != 0:\n",
    "        w_spread =(df['Value$'].multiply(df['AverageSpreadbps'])).sum()/total_val\n",
    "    else:\n",
    "        w_spread = 0\n",
    "\n",
    "    # try:\n",
    "    #     w_spread =(df['Value$'].multiply(df['AverageSpreadbps'])).sum()/total_val\n",
    "    # except RuntimeWarning:\n",
    "    #     w_spread = 0\n",
    "\n",
    "    return pd.Series({\"value_weightedSpread_bps\": w_spread})\n",
    "\n",
    "def mcap_weighted_spread(df):\n",
    "    total_mcap = df['marketCapUsd'].sum()\n",
    "    if total_mcap != 0:\n",
    "        w_spread =(df['marketCapUsd'].multiply(df['AverageSpreadbps'])).sum()/total_mcap\n",
    "    else:\n",
    "        w_spread = 0\n",
    "\n",
    "    return pd.Series({\"mcap_weightedSpread_bps\": w_spread})\n",
    "\n",
    "\n",
    "\n",
    "class Spread:\n",
    "\n",
    "    def __init__(self,names):\n",
    "        self.names = names\n",
    "        self.rics = ticker_fx(self.names)\n",
    "        self.intraday = self.intraday_loader()          #gives the intraday volume - note this does not cumsum. need to call the intraday updater to do this                                                                               #this just gives the 20 day average for particular names passed in      #I'M NOT SURE WE ACTUALLY NEED THIS FUNCTION\n",
    "        self.historic = self.historic_loader()\n",
    "        print(\"initialized spread\")\n",
    "\n",
    "    def update_name_list(self, new_names):\n",
    "        try:\n",
    "            self.names = new_names\n",
    "            self.rics = ticker_fx(new_names)\n",
    "            self.intraday = self.intraday_loader()\n",
    "            self.historic = self.historic_loader()\n",
    "            return True\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "    def intraday_loader(self):\n",
    "\n",
    "        intra = bars(interval=intraday_interval, symlist=self.rics['primaryRic'].tolist(), start_date=dt.now(),\n",
    "                     end_date=dt.now(), startTime=dt(2023, 7, 17, 7).time(), endTime=dt.now().time())\n",
    "        # Now I want a df which combines both the intra dataframe as well as parts of the rics dataframe\n",
    "        intra = intra.merge(self.rics, left_on='sym', right_on='primaryRic', how='left')\n",
    "        # intra = intra['Value','date_time','sym','RATE_TO_USD']\n",
    "        intra['Value$'] = intra['RATE_TO_USD'] * intra['Value'].fillna(method='ffill')\n",
    "        final_df = intra.groupby('date_time').apply(mcap_weighted_spread)  # take the groupby out of here and put it into the intraday updater function\n",
    "        final_df['value_weightedSpread_bps'] = intra.groupby('date_time').apply(value_weighted_spread)\n",
    "\n",
    "        return final_df.reset_index()\n",
    "\n",
    "    def intraday_updater(self):                 # this function again takes in the names we care about, then make a dataframe of the volume data from the last time of the intra df to the current time. it then concatenates it onto the end of the intraday variable. this way, we dont have to load all of the previous days data.\n",
    "        if (dt.now().time() > dt(2023, 8, 14, 8).time()):\n",
    "\n",
    "\n",
    "            last_datetime = max(self.intraday['date_time'])\n",
    "            next = bars(interval=intraday_interval, symlist=self.rics['primaryRic'].tolist(), start_date=last_datetime,end_date=last_datetime, startTime=((last_datetime) - timedelta(hours=1)).time(),endTime=(dt.now() - timedelta(hours=1)).time())\n",
    "            if len(next) == 0:\n",
    "                return self.intraday\n",
    "                       #accounting for the UTC conversion as in the bars function we added on 1 hour anyway. so here we take off an hour.\n",
    "\n",
    "\n",
    "            ## take max value of datetime column, then add 2 mins, then concat to self.intraday\n",
    "            else:\n",
    "                next = next.merge(self.rics, left_on='sym', right_on='primaryRic', how='left')\n",
    "                next['Value$'] = next['RATE_TO_USD'] * next['Value'].fillna(method = 'ffill')\n",
    "                next_final = next.groupby('date_time').apply(mcap_weighted_spread).reset_index()\n",
    "                next_final['Value_weighted'] = next.groupby('date_time').apply(value_weighted_spread)\n",
    "\n",
    "                self.intraday = pd.concat([self.intraday,next_final])\n",
    "                # self.intraday['cum_vol$'] = self.intraday['Value$'].fillna(0).cumsum()\n",
    "                self.intraday = self.intraday[~self.intraday['date_time'].duplicated(keep = 'first')]           #getting rid of duplicated times contaminating the cumsum\n",
    "                # self.intraday = self.intraday[self.intraday['date_time'].dt.time<=dt(2023,7,23,17).time()]\n",
    "                # self.intraday = max(self.intraday['date_time'].time)\n",
    "                self.intraday['mcap_weightedSpread_bps'] = self.intraday['mcap_weightedSpread_bps'].replace(0,pd.NA).fillna(method = 'ffill')\n",
    "                self.intraday['value_weightedSpread_bps'] = self.intraday['value_weightedSpread_bps'].replace(0,pd.NA).fillna(method = 'ffill')\n",
    "                self.intraday= self.intraday[(self.intraday['date_time'].dt.time)<=dt(2023,7,23,16,30).time()]\n",
    "                return self.intraday\n",
    "\n",
    "        else:\n",
    "            last_trade_day = self.historic[self.historic['date'] == self.historic['date'].max()]\n",
    "            # last_trade_day = last_trade_day.merge(self.rics, left_on='sym', right_on='primaryRic', how='left')\n",
    "            last_trade_day['Value$'] = last_trade_day['RATE_TO_USD'] * last_trade_day['Value'].fillna(method='ffill')\n",
    "            final_df = pd.DataFrame()\n",
    "            final_df['mcap_weightedSpread_bps'] = last_trade_day.groupby('date_time').apply(mcap_weighted_spread)  # take the groupby out of here and put it into the intraday updater function\n",
    "            final_df['value_weightedSpread_bps'] = last_trade_day.groupby('date_time').apply(value_weighted_spread)\n",
    "            final_df = final_df.reset_index()\n",
    "            final_df['time'] = final_df['date_time'].dt.time\n",
    "            final_df[final_df['time'] <= dt(2023, 7, 23, 16, 30).time()]\n",
    "            # final_df = final_df[(final_df['date_time'] + dt(2023, 7, 23)).dt.time <= dt(2023, 7, 23, 16, 30).time()]\n",
    "            self.intraday = final_df\n",
    "\n",
    "\n",
    "            return self.intraday\n",
    "\n",
    "\n",
    "\n",
    "    def historic_loader(self):\n",
    "        # rics = ticker_fx(self.names)\n",
    "        prev = bars_prev(interval=historic_interval, symList=self.rics['primaryRic'].tolist(),\n",
    "                         start_date=(dt.now() - timedelta(31)), end_date=(dt.now() - timedelta(days=1)),\n",
    "                         startTime=dt(2023, 7, 17, 7).time(), endTime=dt(2023, 7, 17, 16).time())\n",
    "        first_day = prev['date'].unique()[-20]\n",
    "        prev = prev[prev['date'] >= first_day]\n",
    "        prev = prev.merge(self.rics, left_on='sym', right_on='primaryRic', how='left')\n",
    "        return prev\n",
    "\n",
    "\n",
    "    def historic_updater(self):\n",
    "        current = dt.now()\n",
    "        self.historic = self.historic[self.historic['date_time'] >= (current - timedelta(days=No_day_avg))]\n",
    "        # rics = ticker_fx(self.names)\n",
    "\n",
    "        if max(self.historic['date_time']).date() != (dt.now() - timedelta(days=1)).date():  # checking whether yesterday's data has been added to the self.historics dataframe\n",
    "\n",
    "            days_back = 1\n",
    "            yesterday = bars_prev(interval=historic_interval, symList=self.rics['primaryRic'].tolist(),\n",
    "                                  start_date=dt.now() - timedelta(days=days_back),\n",
    "                                  end_date=dt.now() - timedelta(days=days_back),\n",
    "                                  startTime=dt(2023, 7, 24, 7).time(), endTime=dt(2023, 7, 24, 16).time())\n",
    "\n",
    "            while len(yesterday) != 0:\n",
    "                days_back += 1\n",
    "                yesterday = bars_prev(interval=historic_interval, symList=self.rics['primaryRic'].tolist(),\n",
    "                                      start_date=dt.now() - timedelta(days=days_back),\n",
    "                                      end_date=dt.now() - timedelta(days=days_back),\n",
    "                                      startTime=dt(2023, 7, 24, 7).time(), endTime=dt(2023, 7, 24, 16).time())\n",
    "\n",
    "            yesterday = yesterday.merge(self.rics, left_on='sym', right_on='primaryRic',how='left')  # joining rics on here. we need to do this here so that the yesterday dataframe is in the same form as the sself.historic dataframe. so that we dont mess it up when we concatenate\n",
    "            self.historic = pd.concat([self.historic, yesterday])  # here we concat onto the end of self.historic\n",
    "\n",
    "        self.historic['Value$'] = self.historic['RATE_TO_USD'] * self.historic['Value']  # get dollar value\n",
    "\n",
    "        return self.historic.sort_values(\"date_time\")\n",
    "\n",
    "\n",
    "    def historic_average(self):\n",
    "        t_historic = self.historic_updater()  # historic updater gets called when we click the historic average function\n",
    "        t_historic = t_historic.groupby(['sym', 'time'], as_index=False, sort=False, dropna=False).mean()       #this line returns the mean values for each unique combination of 'sym' and 'time'\n",
    "        t_historic_mcap = t_historic.groupby('time', as_index=True, sort=False, dropna=False).apply(mcap_weighted_spread)  #I have made time the index here\n",
    "        t_historic_value = t_historic.groupby('time', as_index = True).apply(value_weighted_spread)\n",
    "        final = t_historic_mcap.merge(t_historic_value, left_index=True, right_index=True,how = 'inner')\n",
    "        # t_historic_final = t_historic_final.sort_values('time')  # not sure why we sort vals by time here tbh   # This is future TJ I think I've figured it out. Its because the time column isn't in chronological order.\n",
    "        # t_historic_final = t_historic_final.fillna(method='ffill')\n",
    "        # t_historic_final = t_historic_final[(t_historic_final['time']+dt(2023,7,23)).dt.time<=dt(2023,7,23,16,30).time()]\n",
    "        final = final.sort_index()\n",
    "        # final = final.sort_values('time')\n",
    "        final = final.fillna(method='ffill').reset_index()\n",
    "        final = final[(final['time']+dt(2023,7,23)).dt.time<=dt(2023,7,23,16,30).time()]\n",
    "\n",
    "        return final\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
